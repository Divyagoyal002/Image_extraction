{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtoTlcwIRgiq"
      },
      "source": [
        "# **Amazon ML Challenge**\n",
        "\n",
        "\n",
        "---\n",
        "### Problem Overview\n",
        "\n",
        "We have a dataset with the following components:\n",
        "\n",
        "1. **Image Link**: URL to product's image.\n",
        "2. **Category**: The product category.\n",
        "3. **Entity Name**: The attribute whose value has to be predicted (e.g., weight, height).\n",
        "4. **Actual Value**: The true value of the entity (e.g., \"50 grams\", \"15 cm\"). This value is absent in the test data.\n",
        "\n",
        "**Objective**: To train a machine learning model to predict the entity value (eg : \"12 grams\" or \"15 cm\") from the image urls in the test data.\n",
        "\n",
        "**Evaluation Metric**: Predictions will be evaluated using the F1 score, which balances precision and recall.\n",
        "\n",
        "**Constraints**:\n",
        "- Use only specific units provided in the appendix.\n",
        "- Ensure the prediction file format adheres to the validation checker requirements.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H808z-30WF21"
      },
      "source": [
        "\n",
        "# **Our Approach**\n",
        "\n",
        "---\n",
        "## ***1. Text Extraction from Images Using OCR***\n",
        "\n",
        "   - **OCR Technology**: We are using Optical Character Recognition (OCR) technology to extract text data from the images.\n",
        "   - **Tool**: The EasyOCR method is employed to perform this extraction, which converts visual text within images into machine-readable text.\n",
        "\n",
        "## ***2. Data Preprocessing and Entity Extraction***\n",
        "\n",
        "* **Data Preprocessing:** The code involves cleaning and standardizing text data (e.g., replacing commas with periods, handling ambiguous characters).\n",
        "\n",
        "* **Entity Extraction:** The code includes logic for extracting specific entities and their associated values from text based on predefined mappings and patterns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCld9gQ8disj"
      },
      "source": [
        "## ***1. Text Extraction from Images Using OCR***\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqxFkuXKTRpM"
      },
      "source": [
        "## **Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvcPQomVVcFT",
        "outputId": "e41a01f5-2b4e-4d91-d65d-f3dadc02919f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting easyocr\n",
            "  Downloading easyocr-1.7.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from easyocr) (2.4.0+cu121)\n",
            "Requirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.10/dist-packages (from easyocr) (0.19.0+cu121)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from easyocr) (4.10.0.84)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from easyocr) (1.13.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from easyocr) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from easyocr) (9.4.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from easyocr) (0.23.2)\n",
            "Collecting python-bidi (from easyocr)\n",
            "  Downloading python_bidi-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from easyocr) (6.0.2)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.10/dist-packages (from easyocr) (2.0.6)\n",
            "Collecting pyclipper (from easyocr)\n",
            "  Downloading pyclipper-1.3.0.post5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (9.0 kB)\n",
            "Collecting ninja (from easyocr)\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (3.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (2024.6.1)\n",
            "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (2.34.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (2024.8.30)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (24.1)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->easyocr) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->easyocr) (1.3.0)\n",
            "Downloading easyocr-1.7.1-py3-none-any.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyclipper-1.3.0.post5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (908 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m908.3/908.3 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_bidi-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (281 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.3/281.3 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-bidi, pyclipper, ninja, easyocr\n",
            "Successfully installed easyocr-1.7.1 ninja-1.11.1.1 pyclipper-1.3.0.post5 python-bidi-0.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install easyocr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "UeAwCDy9RBM4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import cv2\n",
        "import numpy as np\n",
        "import easyocr\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ff3hjk1UOCX"
      },
      "source": [
        "## **Initialize EasyOCR Reader**\n",
        "Creating a global EasyOCR reader instance to avoid reinitialization for each image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ak4sF7FeUXT5",
        "outputId": "339a3ce9-eca7-494e-bdd8-7ec5667c5189"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:easyocr.easyocr:Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
            "/usr/local/lib/python3.10/dist-packages/easyocr/detection.py:78: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  net.load_state_dict(copyStateDict(torch.load(trained_model, map_location=device)))\n",
            "/usr/local/lib/python3.10/dist-packages/easyocr/recognition.py:169: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(model_path, map_location=device)\n"
          ]
        }
      ],
      "source": [
        "# Global EasyOCR reader to avoid reinitializing for each image\n",
        "reader = easyocr.Reader(['en'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_q1UxtFZUcVh"
      },
      "source": [
        "## **Define Helper Functions**\n",
        "* Downloads an image from the given URL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "AkNjYQbIUiy4"
      },
      "outputs": [],
      "source": [
        "def download_image(url):\n",
        "    response = requests.get(url)\n",
        "    return response.content\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ra96hUBBUm68"
      },
      "source": [
        " * Converts image bytes to an OpenCV image, performs OCR to extract text, and returns the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "vMBnoFbIUq2a"
      },
      "outputs": [],
      "source": [
        "def extract_text(image_bytes):\n",
        "    arr = np.frombuffer(image_bytes, np.uint8)\n",
        "    img = cv2.imdecode(arr, cv2.IMREAD_GRAYSCALE)\n",
        "    result = reader.readtext(img)\n",
        "    return ' '.join([text for _, text, _ in result])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6WmTnktUtad"
      },
      "source": [
        "* Handles the image processing for a single row from the CSV, including downloading and text extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "YFLbG1BoUvP5"
      },
      "outputs": [],
      "source": [
        "def process_row(row):\n",
        "    image_url = row['image_link']\n",
        "    try:\n",
        "        image_bytes = download_image(image_url)\n",
        "        text = extract_text(image_bytes)\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        return f\"Error processing {image_url}: {str(e)}\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9P3lec1UyYv"
      },
      "source": [
        "* Processes a batch of rows concurrently using a ThreadPoolExecutor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "_LRoE6DhUzA3"
      },
      "outputs": [],
      "source": [
        "def process_batch(batch):\n",
        "    results = []\n",
        "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
        "        future_to_row = {executor.submit(process_row, row): row for row in batch.to_dict('records')}\n",
        "        for future in as_completed(future_to_row):\n",
        "            results.append(future.result())\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "envbyw0dVCRr"
      },
      "source": [
        "## **Define the main Function**\n",
        "*  Main function that reads the CSV file, processes it in chunks, and writes the results to a new CSV file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvCvfAICVHPK",
        "outputId": "79ffe49a-19cc-4ffc-9aa7-6f26da4e694f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing complete. Results saved to 'test_data.csv'\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    # Read the CSV file\n",
        "    chunk_size = 2  # Process in smaller chunks\n",
        "    output_file = \"test_data.csv\"\n",
        "\n",
        "    # Write the header to the output file\n",
        "    pd.DataFrame(columns=['image_link', 'extracted_text']).to_csv(output_file, index=False)\n",
        "\n",
        "    # Process the full CSV file in chunks\n",
        "    for chunk in pd.read_csv(\"test.csv\", chunksize=chunk_size, nrows=6):\n",
        "        extracted_texts = process_batch(chunk)\n",
        "        chunk['extracted_text'] = extracted_texts\n",
        "\n",
        "        # Append the results to the CSV file\n",
        "        chunk.to_csv(output_file, mode='a', header=False, index=False)\n",
        "\n",
        "\n",
        "    print(f\"Processing complete. Results saved to '{output_file}'\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2CtyigJaI0_"
      },
      "source": [
        "## ***2. Data Preprocessing and Entity Extraction***\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "h9sy_9SiV5HR"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('train_data.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDANiu7QaqXT"
      },
      "source": [
        "## **Define Preprocessing Function**\n",
        "\n",
        "* Replace commas with periods in numeric values within text strings to standardize numeric formats.\n",
        "* Replace ambiguous characters in the text to clean and standardize it for further processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "sHriMeGcbD-Q"
      },
      "outputs": [],
      "source": [
        "def replace_comma_with_period(text):\n",
        "    if isinstance(text, str):  # Ensure the input is a string\n",
        "        return re.sub(r'(\\d),(\\d)', r'\\1.\\2', text)\n",
        "    return text  # Return unchanged if not a string\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "pR5WMcNvbLHz"
      },
      "outputs": [],
      "source": [
        "# Define Replacement for Ambiguous Characters\n",
        "replacements = {\n",
        "    'O': '0',\n",
        "    'D': '0',\n",
        "    'l': '1',\n",
        "    'I': '1',\n",
        "    'S': '5',\n",
        "    'B': '8',\n",
        "    'Z': '2',\n",
        "    's': '5'\n",
        "}\n",
        "# Define Preprocessing Function for Text\n",
        "def preprocess_text(text):\n",
        "    for key, value in replacements.items():\n",
        "        text = text.replace(key, value)\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "QlhEc6dXb9Re"
      },
      "outputs": [],
      "source": [
        "df['corrected_text'] = df['extracted_text'].apply(replace_comma_with_period)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAzwhNDKbpg5"
      },
      "source": [
        "## **Define Entity-to-Unit Mapping**\n",
        "\n",
        "*   Map entities (e.g., width, height) to their possible units for later extraction.\n",
        "*   Provide mappings from abbreviated or symbolic unit representations to their full names for normalization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "5qjTI9t8bzgg"
      },
      "outputs": [],
      "source": [
        "# Entity to unit mapping\n",
        "entity_unit_map = {\n",
        "    'width': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
        "    'depth': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
        "    'height': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
        "    'item_weight': {'gram', 'kilogram', 'microgram', 'milligram', 'ounce', 'pound', 'ton'},\n",
        "    'maximum_weight_recommendation': {'gram', 'kilogram', 'microgram', 'milligram', 'ounce', 'pound', 'ton'},\n",
        "    'voltage': {'kilovolt', 'millivolt', 'volt'},\n",
        "    'wattage': {'kilowatt', 'watt'},\n",
        "    'item_volume': {'centilitre', 'cubic foot', 'cubic inch', 'cup', 'decilitre', 'fluid ounce',\n",
        "                    'gallon', 'imperial gallon', 'litre', 'microlitre', 'millilitre', 'pint', 'quart'}\n",
        "}\n",
        "\n",
        "# Symbol to unit conversion\n",
        "unit_symbol_map = {\n",
        "    'g': 'gram',\n",
        "    'kg': 'kilogram',\n",
        "    'k9': 'kilogram',\n",
        "    'mg': 'milligram',\n",
        "    'm9': 'milligram',\n",
        "    'lb': 'pound',\n",
        "    '1b': 'pound',\n",
        "    'Ib': 'pound',\n",
        "    '%und': 'pound',\n",
        "    '% und': 'pound',\n",
        "    'oz': 'ounce',\n",
        "    'o2': 'ounce',\n",
        "    '0z': 'ounce',\n",
        "    'ml': 'millilitre',\n",
        "    'mI': 'millilitre',\n",
        "    'm1': 'millilitre',\n",
        "    'l': 'litre',\n",
        "    'cm': 'centimetre',\n",
        "    'mm': 'millimetre',\n",
        "    'm': 'metre',\n",
        "    'kv': 'kilovolt',\n",
        "    'v': 'volt',\n",
        "    'w': 'watt'\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VB93YJocij8"
      },
      "source": [
        "## **Function to Attempt Extraction of Values**\n",
        "\n",
        "Use regex to find and normalize values and units in the text, then return the most appropriate match based on valid units for the given entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "XgKIa8-jcR7J"
      },
      "outputs": [],
      "source": [
        "def extract_entity_value(row):\n",
        "    entity = row['entity_name']\n",
        "    text = row['corrected_text']\n",
        "\n",
        "    # Ensure the text is a string\n",
        "    if pd.isna(text):\n",
        "        return None\n",
        "    text = str(text)\n",
        "\n",
        "    # First attempt to extract without preprocessing\n",
        "    extracted_value = attempt_extraction(text, entity)\n",
        "\n",
        "    # If no value is extracted, apply the replacement method and try again\n",
        "    if extracted_value is None:\n",
        "        text = preprocess_text(text)\n",
        "        extracted_value = attempt_extraction(text, entity)\n",
        "\n",
        "    return extracted_value\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbX4AkCvcGqA"
      },
      "source": [
        "## **Function to Extract Entity Value**\n",
        "Extract relevant entity values from the text, first without and then with preprocessing if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "qozzXkuDcWUT"
      },
      "outputs": [],
      "source": [
        "def attempt_extraction(text, entity):\n",
        "    # Get valid units for the entity\n",
        "    valid_units = entity_unit_map.get(entity, set())\n",
        "\n",
        "    # Create a regex pattern for matching numbers followed by units\n",
        "    unit_pattern = '|'.join([re.escape(unit) for unit in valid_units] + [re.escape(symbol) for symbol in unit_symbol_map.keys()])\n",
        "    pattern = fr'(\\d+(?:\\.\\d+)?)\\s*({unit_pattern})'\n",
        "\n",
        "    # Find all matches in the text\n",
        "    matches = re.findall(pattern, text, re.IGNORECASE)\n",
        "\n",
        "    # If matches are found, process them\n",
        "    extracted_values = []\n",
        "    for match in matches:\n",
        "        value, unit = match\n",
        "\n",
        "        # Normalize unit (if symbol is used, convert to full unit name)\n",
        "        normalized_unit = unit_symbol_map.get(unit.lower(), unit.lower())\n",
        "\n",
        "        # Check if the normalized unit is in the valid units set\n",
        "        if normalized_unit in valid_units:\n",
        "            extracted_values.append(f\"{value} {normalized_unit}\")\n",
        "\n",
        "    # Return the most appropriate match (e.g., first one found)\n",
        "    if extracted_values:\n",
        "        return extracted_values[0]\n",
        "\n",
        "    return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "fqYnaNuvcyEe"
      },
      "outputs": [],
      "source": [
        "  df['extracted_entity_value'] = df.apply(extract_entity_value, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoIlWVz0cP0t"
      },
      "source": [
        "## **Prepare and Save the Output DataFrame**\n",
        "Create a DataFrame with extracted entity values as predictions, save it to a CSV file, and print the final output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCuTlGF7c_h1",
        "outputId": "f8e98995-b172-4090-fcd3-95b018f2071a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     index      prediction\n",
            "0        0        500 gram\n",
            "1        1    5 millilitre\n",
            "2        2      0.709 gram\n",
            "3        3            None\n",
            "4        4  1400 milligram\n",
            "..     ...             ...\n",
            "894    894  240 millilitre\n",
            "895    895  1485 milligram\n",
            "896    896    40 milligram\n",
            "897    897            None\n",
            "898    898            None\n",
            "\n",
            "[899 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "output_df = df[['extracted_entity_value']].rename(columns={'extracted_entity_value': 'prediction'})\n",
        "output_df.index.name = 'index'\n",
        "output_df.reset_index(inplace=True)\n",
        "\n",
        "# Save the output to a CSV file\n",
        "output_df.to_csv('output.csv', index=False)\n",
        "\n",
        "# Show the final DataFrame with predictions\n",
        "print(output_df)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
